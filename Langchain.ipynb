{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8477405,"sourceType":"datasetVersion","datasetId":5055848}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain langchain_community\n!pip install transformers\n!pip install sentence-transformers\n!pip install bitsandbytes accelerate\n!pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:41:13.039316Z","iopub.execute_input":"2024-05-21T13:41:13.039694Z","iopub.status.idle":"2024-05-21T13:43:17.999451Z","shell.execute_reply.started":"2024-05-21T13:41:13.039662Z","shell.execute_reply":"2024-05-21T13:43:17.998320Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\nCollecting langchain_community\n  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nCollecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_core-0.2.0-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.60-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.2.0-py3-none-any.whl (973 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\nDownloading langsmith-0.1.60-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.0 langchain-core-0.2.0 langchain-text-splitters-0.2.0 langchain_community-0.2.0 langsmith-0.1.60 orjson-3.10.3 packaging-23.2\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.2.75.tar.gz (48.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.75-cp310-cp310-linux_x86_64.whl size=3710719 sha256=7b4e5c4fbdc08e833f6fd497746c8005aedc975c9766f294249f3e312e2e4d9b\n  Stored in directory: /root/.cache/pip/wheels/5e/df/9a/e4bb2e48bfa64fb174f0f786296c8507dbebea2a112f1adf8d\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.75\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.llms import LlamaCpp\n\nllm_cpp = LlamaCpp(\n            streaming = True,\n            model_path=\"/kaggle/input/siemens-langchain/zephyr-7b-beta.Q4_0.gguf\",\n            n_gpu_layers=2,\n            n_batch=512,\n            temperature=0.75,\n            top_p=1,\n            verbose=True,\n            n_ctx=4096\n            )","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:44:44.550408Z","iopub.execute_input":"2024-05-21T13:44:44.551189Z","iopub.status.idle":"2024-05-21T13:45:13.943964Z","shell.execute_reply.started":"2024-05-21T13:44:44.551156Z","shell.execute_reply":"2024-05-21T13:45:13.942896Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /kaggle/input/siemens-langchain/zephyr-7b-beta.Q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.15 MiB\nllm_load_tensors:        CPU buffer size =  3917.87 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\nllama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\nUsing fallback chat format: llama-2\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Who is Elon Musk?\"\n\nprompt = f\"\"\"\n <|system|>\nYou are an AI assistant that follows instruction extremely well.\nPlease be truthful and give direct answers\n</s>\n <|user|>\n {query}\n </s>\n <|assistant|>\n\"\"\"\n\nresponse = llm_cpp.predict(prompt)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:45:13.945928Z","iopub.execute_input":"2024-05-21T13:45:13.946341Z","iopub.status.idle":"2024-05-21T13:47:09.248212Z","shell.execute_reply.started":"2024-05-21T13:45:13.946313Z","shell.execute_reply":"2024-05-21T13:47:09.247324Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n  warn_deprecated(\n\nllama_print_timings:        load time =   10462.04 ms\nllama_print_timings:      sample time =     144.11 ms /   256 runs   (    0.56 ms per token,  1776.42 tokens per second)\nllama_print_timings: prompt eval time =   10461.80 ms /    56 tokens (  186.82 ms per token,     5.35 tokens per second)\nllama_print_timings:        eval time =  103477.57 ms /   255 runs   (  405.79 ms per token,     2.46 tokens per second)\nllama_print_timings:       total time =  115252.09 ms /   311 tokens\n","output_type":"stream"},{"name":"stdout","text":"Elon Musk is a South African-born entrepreneur, engineer, and business magnate. He is the founder, CEO, and Chief Engineer at SpaceX; the CEO and largest individual shareholder of Tesla, Inc.; the founder, Chairman, and CEO of Neuralink; the co-founder and Initial Chairperson of SolarCity; and a co-founder of PayPal. Musk has been called a real-life Tony Stark (Iron Man) because of his innovative and futuristic ideas for sustainable energy, renewable resources, transportation, infrastructure, and space exploration.\n\nMusk's career began with the creation of Zip2, an online business directory, which was acquired by Compaq Computer Corporation in 1999 for $307 million. Musk then founded X.com, an online payment company that merged with PayPal in 2000, earning Musk around $165 million. He has since created several other companies, including SpaceX, Tesla, and The Boring Company, which are all leaders in their respective industries. Musk's work in renewable energy and space exploration aims to address some of the world's most pressing challenges,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prompt","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\nprompt_template_name = PromptTemplate(\n    input_variables =['cuisine'],\n    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n)\np = prompt_template_name.format(cuisine=\"Italian\")\nprint(p)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:52:04.189464Z","iopub.execute_input":"2024-05-21T13:52:04.190129Z","iopub.status.idle":"2024-05-21T13:52:04.344744Z","shell.execute_reply.started":"2024-05-21T13:52:04.190097Z","shell.execute_reply":"2024-05-21T13:52:04.343812Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"I want to open a restaurant for Italian food. Suggest a fency name for this.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.chains import LLMChain\n\n\nprompt_template_name = PromptTemplate(\n    input_variables =['cuisine'],\n    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n)\n\nname_chain =LLMChain(llm=llm_cpp, prompt=prompt_template_name, output_key=\"restaurant_name\")\n\n\nprompt_template_items = PromptTemplate(\n    input_variables = ['restaurant_name'],\n    template=\"Suggest some menu items for {restaurant_name}.\"\n)\n\nfood_items_chain =LLMChain(llm=llm_cpp, prompt=prompt_template_items, output_key=\"menu_items\")","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:58:21.835684Z","iopub.execute_input":"2024-05-21T13:58:21.836387Z","iopub.status.idle":"2024-05-21T13:58:21.842520Z","shell.execute_reply.started":"2024-05-21T13:58:21.836356Z","shell.execute_reply":"2024-05-21T13:58:21.841581Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import SequentialChain\n\nchain = SequentialChain(\n    chains = [name_chain, food_items_chain],\n    input_variables = ['cuisine'],\n    output_variables = ['restaurant_name', \"menu_items\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:58:33.066151Z","iopub.execute_input":"2024-05-21T13:58:33.066571Z","iopub.status.idle":"2024-05-21T13:58:33.079881Z","shell.execute_reply.started":"2024-05-21T13:58:33.066528Z","shell.execute_reply":"2024-05-21T13:58:33.078987Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"chain({\"cuisine\": \"Indian\"})","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:59:10.298928Z","iopub.execute_input":"2024-05-21T13:59:10.299291Z","iopub.status.idle":"2024-05-21T14:01:26.780442Z","shell.execute_reply.started":"2024-05-21T13:59:10.299262Z","shell.execute_reply":"2024-05-21T14:01:26.779579Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n  warn_deprecated(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =   10462.04 ms\nllama_print_timings:      sample time =      27.99 ms /    54 runs   (    0.52 ms per token,  1929.47 tokens per second)\nllama_print_timings: prompt eval time =    2319.31 ms /    12 tokens (  193.28 ms per token,     5.17 tokens per second)\nllama_print_timings:        eval time =   20830.25 ms /    53 runs   (  393.02 ms per token,     2.54 tokens per second)\nllama_print_timings:       total time =   23398.93 ms /    65 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =   10462.04 ms\nllama_print_timings:      sample time =     149.82 ms /   256 runs   (    0.59 ms per token,  1708.75 tokens per second)\nllama_print_timings: prompt eval time =   11013.06 ms /    60 tokens (  183.55 ms per token,     5.45 tokens per second)\nllama_print_timings:        eval time =  100731.18 ms /   255 runs   (  395.02 ms per token,     2.53 tokens per second)\nllama_print_timings:       total time =  113047.03 ms /   315 tokens\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'cuisine': 'Indian',\n 'restaurant_name': '\\n Also, provide some suggestions regarding the decor and ambience of the place that would go well with the name you suggest. \\n\\nAlso, what are some must-have dishes in an Indian restaurant and how can they be presented to impress the customers?',\n 'menu_items': '\\n\\nAnswer:\\n\\nMenu Items:\\n1. Spicy Chicken Tikka - Grilled chicken marinated in a blend of spices, served with freshly squeezed lime juice and mint yogurt dip\\n2. Butter Paneer Masala - Soft Indian cheese cooked in a creamy tomato-based gravy flavored with aromatic spices\\n3. Chicken Biryani - A fragrant rice dish mixed with tender chicken pieces, saffron, and spices\\n4. Tandoori Fish - Grilled fish marinated in a blend of spices, served with lemon wedges and cucumber raita\\n5. Mango Lassi - A refreshing drink made with fresh mango puree, yogurt, and a touch of honey\\n6. Gulab Jamun - Deep-fried milk balls soaked in a sugary syrup flavored with rose water and cardamom\\n\\nDecor and Ambience:\\n1. Use earthy tones like ochre, rust, and brown to create a warm and inviting atmosphere\\n2. Hang lanterns in various shapes and sizes to add a touch of Indian culture\\n3. Display traditional'}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}