{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8477405,"sourceType":"datasetVersion","datasetId":5055848},{"sourceId":8493052,"sourceType":"datasetVersion","datasetId":5067286}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain langchain_community\n!pip install transformers\n!pip install sentence-transformers\n!pip install bitsandbytes accelerate\n!pip install llama-cpp-python\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, SequentialChain","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:48:15.184433Z","iopub.execute_input":"2024-05-23T06:48:15.184832Z","iopub.status.idle":"2024-05-23T06:48:15.809409Z","shell.execute_reply.started":"2024-05-23T06:48:15.184799Z","shell.execute_reply":"2024-05-23T06:48:15.808626Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nllm_cpp = LlamaCpp(\n            streaming = True,\n            model_path=\"/kaggle/input/siemens-langchain/zephyr-7b-beta.Q4_0.gguf\",\n            n_gpu_layers=4,\n            n_batch=512,\n            temperature=0.6,\n            top_p=1,\n            verbose=True,\n            n_ctx=4096\n            )","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:54:31.416122Z","iopub.execute_input":"2024-05-23T06:54:31.416956Z","iopub.status.idle":"2024-05-23T06:54:32.176526Z","shell.execute_reply.started":"2024-05-23T06:54:31.416924Z","shell.execute_reply":"2024-05-23T06:54:32.175607Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /kaggle/input/siemens-langchain/zephyr-7b-beta.Q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.15 MiB\nllm_load_tensors:        CPU buffer size =  3917.87 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\nllama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\nUsing fallback chat format: llama-2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prompt","metadata":{}},{"cell_type":"code","source":"\nprompt_template_name = PromptTemplate(\n    input_variables =['cuisine'],\n    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n)\np = prompt_template_name.format(cuisine=\"Italian\")\nprint(p)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import LLMChain\n\n\nprompt_template_name = PromptTemplate(\n    input_variables =['cuisine'],\n    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n)\n\nname_chain =LLMChain(llm=llm_cpp, prompt=prompt_template_name, output_key=\"restaurant_name\")\n\n\nprompt_template_items = PromptTemplate(\n    input_variables = ['restaurant_name'],\n    template=\"Suggest some menu items for {restaurant_name}.\"\n)\n\nfood_items_chain =LLMChain(llm=llm_cpp, prompt=prompt_template_items, output_key=\"menu_items\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:53:45.419544Z","iopub.execute_input":"2024-05-23T06:53:45.419938Z","iopub.status.idle":"2024-05-23T06:53:45.426841Z","shell.execute_reply.started":"2024-05-23T06:53:45.419909Z","shell.execute_reply":"2024-05-23T06:53:45.425861Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import SequentialChain\n\nchain = SequentialChain(\n    chains = [name_chain, food_items_chain],\n    input_variables = ['cuisine'],\n    output_variables = ['restaurant_name', \"menu_items\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain({\"cuisine\": \"Indian\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install streamlit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir(chain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SerpAPI","metadata":{}},{"cell_type":"code","source":"import os\n#!pip install google-search-results\n\nos.environ['SERPAPI_API_KEY'] = \"27c61505166ceb4302b1e07f6169cf412a042c29d301e449d56b1f781cf5abb0\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.agents import AgentType, initialize_agent, load_tools\n\n\n# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm_cpp)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(tools, llm_cpp, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors = True)\n\n# Let's test it out!\nagent.run(\"What was the GDP of US in 2022 plus 5?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_template_name","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:52:43.779649Z","iopub.execute_input":"2024-05-23T06:52:43.780038Z","iopub.status.idle":"2024-05-23T06:52:43.787189Z","shell.execute_reply.started":"2024-05-23T06:52:43.780009Z","shell.execute_reply":"2024-05-23T06:52:43.786332Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"PromptTemplate(input_variables=['cuisine'], template='I want to open a restaurant for {cuisine} food. Suggest one name only.')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Memory","metadata":{}},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\n\nchain = LLMChain(llm=llm_cpp, prompt=prompt_template_name, memory=memory)\nname = chain.run(\"Mexican\")\nprint(name)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:55:22.236137Z","iopub.execute_input":"2024-05-23T06:55:22.236489Z","iopub.status.idle":"2024-05-23T06:57:07.759407Z","shell.execute_reply.started":"2024-05-23T06:55:22.236462Z","shell.execute_reply":"2024-05-23T06:57:07.758448Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\nllama_print_timings:        load time =    3588.12 ms\nllama_print_timings:      sample time =     144.87 ms /   256 runs   (    0.57 ms per token,  1767.05 tokens per second)\nllama_print_timings: prompt eval time =    3588.00 ms /    20 tokens (  179.40 ms per token,     5.57 tokens per second)\nllama_print_timings:        eval time =  100642.71 ms /   255 runs   (  394.68 ms per token,     2.53 tokens per second)\nllama_print_timings:       total time =  105506.17 ms /   275 tokens\n","output_type":"stream"},{"name":"stdout","text":"\n\nWe will suggest you some fency names for your Mexican food restaurant based on the type of cuisine served, the location, and the target audience. Here are our suggestions:\n\n1. Estancia Azteca - This name combines the Spanish word \"estancia,\" which means estate or place, with \"Azteca\" to represent the rich cultural heritage of Mexico's indigenous people. It has a sophisticated and elegant sound that would appeal to an upscale clientele.\n\n2. La Hacienda de Oro - This name translates to \"The Golden Ranch\" in Spanish, which evokes images of luxury and opulence. The word \"hacienda\" also has connotations of tradition and history, making it a fitting name for a Mexican restaurant that serves authentic dishes with a modern twist.\n\n3. Tequila Luna - This name combines the words \"tequila,\" which is a popular Mexican spirit, with \"luna,\" which means moon in Spanish. It has a sophisticated and elegant sound that would appeal to an upscale clientele, and it also conveys a sense of mystery and allure.\n\n4. Casa de las Flores - This name translates\n","output_type":"stream"}]},{"cell_type":"code","source":"print(chain.memory.buffer)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:57:15.458356Z","iopub.execute_input":"2024-05-23T06:57:15.458757Z","iopub.status.idle":"2024-05-23T06:57:15.463885Z","shell.execute_reply.started":"2024-05-23T06:57:15.458725Z","shell.execute_reply":"2024-05-23T06:57:15.462897Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Human: Mexican\nAI: \n\nWe will suggest you some fency names for your Mexican food restaurant based on the type of cuisine served, the location, and the target audience. Here are our suggestions:\n\n1. Estancia Azteca - This name combines the Spanish word \"estancia,\" which means estate or place, with \"Azteca\" to represent the rich cultural heritage of Mexico's indigenous people. It has a sophisticated and elegant sound that would appeal to an upscale clientele.\n\n2. La Hacienda de Oro - This name translates to \"The Golden Ranch\" in Spanish, which evokes images of luxury and opulence. The word \"hacienda\" also has connotations of tradition and history, making it a fitting name for a Mexican restaurant that serves authentic dishes with a modern twist.\n\n3. Tequila Luna - This name combines the words \"tequila,\" which is a popular Mexican spirit, with \"luna,\" which means moon in Spanish. It has a sophisticated and elegant sound that would appeal to an upscale clientele, and it also conveys a sense of mystery and allure.\n\n4. Casa de las Flores - This name translates\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## CHATBOT","metadata":{}},{"cell_type":"code","source":"from langchain.chains import ConversationChain\n\nconvo = ConversationChain(llm=llm_cpp)\nprint(convo.prompt.template)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:57:25.712695Z","iopub.execute_input":"2024-05-23T06:57:25.713101Z","iopub.status.idle":"2024-05-23T06:57:25.720055Z","shell.execute_reply.started":"2024-05-23T06:57:25.713068Z","shell.execute_reply":"2024-05-23T06:57:25.718913Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:\n","output_type":"stream"}]},{"cell_type":"code","source":"convo.run(\"Who won the first cricket world cup?\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:57:29.083853Z","iopub.execute_input":"2024-05-23T06:57:29.084219Z","iopub.status.idle":"2024-05-23T06:59:22.747545Z","shell.execute_reply.started":"2024-05-23T06:57:29.084190Z","shell.execute_reply":"2024-05-23T06:59:22.746651Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    3588.12 ms\nllama_print_timings:      sample time =     143.34 ms /   256 runs   (    0.56 ms per token,  1785.94 tokens per second)\nllama_print_timings: prompt eval time =   13488.59 ms /    70 tokens (  192.69 ms per token,     5.19 tokens per second)\nllama_print_timings:        eval time =   98864.50 ms /   255 runs   (  387.70 ms per token,     2.58 tokens per second)\nllama_print_timings:       total time =  113647.33 ms /   325 tokens\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"\" Blessed with a rich cricketing history dating back over two centuries, the sport of cricket has witnessed some exhilarating moments in international tournaments. The first-ever ICC Cricket World Cup was held in England in 1975. The tournament featured eight teams, namely, Australia, Bangladesh (then East Pakistan), England, India, New Zealand, Pakistan, South Africa, and the West Indies.\\n\\nAfter a round-robin group stage, the teams were divided into two groups based on their points tally. Group A comprised of Australia, New Zealand, Pakistan, and the West Indies, while Group B featured England, India, South Africa, and Bangladesh (then East Pakistan).\\n\\nThe Super Sixes stage followed, where each team played against the other three teams from a different group. The top two teams from this stage progressed to the semi-finals.\\n\\nIn the semi-finals, Australia defeated New Zealand by 47 runs, while the West Indies beat Pakistan by four wickets. In the final, held at the iconic Lord's Cricket Ground in London on June 21, 1975, Clive Lloyd's West Ind\""},"metadata":{}}]},{"cell_type":"code","source":"# restricting convo buffer size\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k=1)\nconvo = ConversationChain(llm=llm_cpp, memory=memory)\nconvo.run(\"5+5\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T07:00:59.274000Z","iopub.execute_input":"2024-05-23T07:00:59.274406Z","iopub.status.idle":"2024-05-23T07:02:40.614564Z","shell.execute_reply.started":"2024-05-23T07:00:59.274375Z","shell.execute_reply":"2024-05-23T07:02:40.613662Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time =    3588.12 ms\nllama_print_timings:      sample time =     138.62 ms /   256 runs   (    0.54 ms per token,  1846.74 tokens per second)\nllama_print_timings: prompt eval time =    1398.58 ms /     7 tokens (  199.80 ms per token,     5.01 tokens per second)\nllama_print_timings:        eval time =   98632.78 ms /   255 runs   (  386.80 ms per token,     2.59 tokens per second)\nllama_print_timings:       total time =  101322.07 ms /   262 tokens\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"' The sum of five and five is ten. Ten is a number that comes after nine in a sequence called counting, and before twelve in the same sequence. It is also a digit used to represent the number ten when writing numerals in certain number systems such as decimal system. In binary notation, it is written as 1010. In ternary notation, it is written as 102. In quaternary notation, it is written as 23.\\nIn mathematics, ten is the base used in the decimal system, which is also referred to as denary system or base-ten numeral system. It is commonly used for counting and measuring because it has just enough digits to be convenient for everyday use while still being small enough to be handled by the human mind without major difficulties.\\nIn many languages, the word for ten in native numerals is derived from Latin decem, with cognates such as German zehn, Dutch tien, Danish tyve, Norwegian ti, Swedish tio, French dix, Romanian zece, Polish dziesięć, Lithuanian desimtis, Latvian desmit, Estonian kümme, and Russian десять'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}